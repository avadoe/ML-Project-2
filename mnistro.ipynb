{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-04-30T07:32:55.874833Z",
     "iopub.execute_input": "2023-04-30T07:32:55.876190Z",
     "iopub.status.idle": "2023-04-30T07:32:55.908890Z",
     "shell.execute_reply.started": "2023-04-30T07:32:55.876147Z",
     "shell.execute_reply": "2023-04-30T07:32:55.907927Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "datatransform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    \"../MNIST_Train\", train=True, download=True, transform=datatransform\n",
    ")\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    \"../MNIST_Test\", train=False, download=True, transform=datatransform\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "def one_hot_encode(label: int) -> torch.Tensor:\n",
    "    x = torch.zeros([label.shape[0], num_classes], dtype=torch.float32)\n",
    "    for i in range(label.shape[0]):\n",
    "        x[i, label[i]] = 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, activation,) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=16,kernel_size=5,stride=3)\n",
    "        self.actv = activation()\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=16*8*8,out_features=hidden_size),\n",
    "            nn.Linear(in_features=hidden_size,out_features=hidden_size),\n",
    "            nn.Linear(in_features=hidden_size,out_features=num_classes),\n",
    "        ) if (num_layers == 3) else nn.Sequential(\n",
    "            nn.Linear(in_features=16*8*8,out_features=hidden_size),\n",
    "            nn.Linear(in_features=hidden_size,out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # input(x.shape)\n",
    "        x = self.conv(x)\n",
    "        # input(x.shape)\n",
    "        x = self.actv(x)\n",
    "        # input(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # input(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # input(x.shape)\n",
    "        return x\n",
    "\n",
    "# Combinations\n",
    "\n",
    "model_list = [\n",
    "    {'num_layers': 2, 'hidden_size': 100, 'activation': nn.Tanh},\n",
    "    {'num_layers': 2, 'hidden_size': 100, 'activation': nn.Sigmoid},\n",
    "    {'num_layers': 2, 'hidden_size': 100, 'activation': nn.ReLU},\n",
    "    {'num_layers': 2, 'hidden_size': 150, 'activation': nn.Tanh},\n",
    "    {'num_layers': 2, 'hidden_size': 150, 'activation': nn.Sigmoid},\n",
    "    {'num_layers': 2, 'hidden_size': 150, 'activation': nn.ReLU},\n",
    "    {'num_layers': 3, 'hidden_size': 100, 'activation': nn.Tanh},\n",
    "    {'num_layers': 3, 'hidden_size': 100, 'activation': nn.Sigmoid},\n",
    "    {'num_layers': 3, 'hidden_size': 100, 'activation': nn.ReLU},\n",
    "    {'num_layers': 3, 'hidden_size': 150, 'activation': nn.Tanh},\n",
    "    {'num_layers': 3, 'hidden_size': 150, 'activation': nn.Sigmoid},\n",
    "    {'num_layers': 3, 'hidden_size': 150, 'activation': nn.ReLU},\n",
    "]\n",
    "\n",
    "optimizer_list = [optim.SGD, optim.Adam]\n",
    "\n",
    "\n",
    "for model_dict in model_list:\n",
    "\n",
    "    model = Net(**model_dict)\n",
    "\n",
    "    num_layers = model_dict['num_layers']\n",
    "    hidden_size = model_dict['hidden_size']\n",
    "    activation_func = {nn.Tanh: \"tanh\", nn.ReLU: \"relu\", nn.Sigmoid: \"sigmoid\"}[model_dict['activation']]\n",
    "\n",
    "    for optimizer_func in optimizer_list:\n",
    "\n",
    "        optimizer_fn = {optim.Adam: \"Adam\", optim.SGD: \"SGD\"}[optimizer_func]\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optimizer_func(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        training_loss = 0.0\n",
    "        training_acc = 0.0\n",
    "\n",
    "        print(f\"+{'-'*36:36s}+\")\n",
    "        print(f\"|{f'TRAINING: ({num_layers},{hidden_size},{activation_func},{optimizer_fn})':^36s}|\")\n",
    "        print(f\"+{'-'*10:^10s}+{'-'*12:^12s}+{'-'*12:^12s}+\")\n",
    "        print(f\"|{'Epoch':^10s}|{'Loss':^12s}|{'Accuracy':^12s}|\")\n",
    "        print(f\"+{'-'*10:^10s}+{'-'*12:^12s}+{'-'*12:^12s}+\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, d in enumerate(trainloader):\n",
    "                tensor, label = d  # Shapes - (4,1,28,28) # (4)\n",
    "                output = model(tensor)\n",
    "                target = one_hot_encode(label)\n",
    "                acc = torch.sum(torch.argmax(output,-1)==torch.argmax(target,1))\n",
    "                training_acc += acc.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(output,target)\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                del loss\n",
    "            training_loss /= float(len(trainloader.dataset))\n",
    "            training_acc /= float(len(trainloader.dataset))\n",
    "            print(f\"|{f'{epoch+1:02d}/{num_epochs:02d}':^10s}|{f'{training_loss:.6f}':^12s}|{f'{training_acc:.6f}':^12s}|\")\n",
    "\n",
    "        print(f\"+{'-'*10:^10s}+{'-'*12:^12s}+{'-'*12:^12s}+\\n\")\n",
    "\n",
    "        # Testing\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        print(f\"+{'-'*25:25s}+\")\n",
    "        print(f\"|{'TESTING':^25s}|\")\n",
    "        print(f\"+{'-'*12:^12s}+{'-'*12:^12s}+\")\n",
    "        print(f\"|{'Loss':^12s}|{'Accuracy':^12s}|\")\n",
    "        print(f\"+{'-'*12:^12s}+{'-'*12:^12s}+\")\n",
    "\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for i, d in enumerate(trainloader):\n",
    "                tensor, label = d  # Shapes - (4,1,28,28) # (4)\n",
    "                output = model(tensor)\n",
    "                target = one_hot_encode(label)\n",
    "                acc = torch.sum(torch.argmax(output,-1)==torch.argmax(target,1))\n",
    "                val_acc += acc.item()\n",
    "                loss = criterion(output,target)\n",
    "                val_loss += loss.item()\n",
    "                del loss\n",
    "            val_loss /= float(len(trainloader.dataset))\n",
    "            val_acc /= float(len(trainloader.dataset))\n",
    "            print(f\"|{f'{val_loss:.6f}':^12s}|{f'{val_acc:.6f}':^12s}|\")\n",
    "\n",
    "        print(f\"+{'-'*12:^12s}+{'-'*12:^12s}+\\n\\n\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-30T07:32:55.912510Z",
     "iopub.execute_input": "2023-04-30T07:32:55.912790Z",
     "iopub.status.idle": "2023-04-30T08:40:36.211722Z",
     "shell.execute_reply.started": "2023-04-30T07:32:55.912763Z",
     "shell.execute_reply": "2023-04-30T08:40:36.210457Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../MNIST_Train/MNIST/raw/train-images-idx3-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64bb05a8ccf54769b367ebcc30a75aff"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Train/MNIST/raw/train-images-idx3-ubyte.gz to ../MNIST_Train/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../MNIST_Train/MNIST/raw/train-labels-idx1-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "083c237eaa3a4f6caed8c3b32a5f794b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Train/MNIST/raw/train-labels-idx1-ubyte.gz to ../MNIST_Train/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../MNIST_Train/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90435a2a414947bc84a124efbe3212da"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Train/MNIST/raw/t10k-images-idx3-ubyte.gz to ../MNIST_Train/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../MNIST_Train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "799280f38a0d485290141679475b067a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Train/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../MNIST_Train/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../MNIST_Test/MNIST/raw/train-images-idx3-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61adf8e0d247402ab73f9f713a5de427"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Test/MNIST/raw/train-images-idx3-ubyte.gz to ../MNIST_Test/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../MNIST_Test/MNIST/raw/train-labels-idx1-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1064600a832c4603a915fb046dedb602"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Test/MNIST/raw/train-labels-idx1-ubyte.gz to ../MNIST_Test/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../MNIST_Test/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d8a2d0b03294356891f7c919dd1d19f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Test/MNIST/raw/t10k-images-idx3-ubyte.gz to ../MNIST_Test/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../MNIST_Test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c3af520423944d2bd015fa9d7aa07c5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Extracting ../MNIST_Test/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../MNIST_Test/MNIST/raw\n\n+------------------------------------+\n|     TRAINING: (2,100,tanh,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.053824  |  0.580983  |\n|  02/10   |  0.025240  |  0.821160  |\n|  03/10   |  0.017188  |  0.857497  |\n|  04/10   |  0.014423  |  0.874064  |\n|  05/10   |  0.013037  |  0.882581  |\n|  06/10   |  0.012186  |  0.888981  |\n|  07/10   |  0.011596  |  0.893681  |\n|  08/10   |  0.011162  |  0.897648  |\n|  09/10   |  0.010816  |  0.900148  |\n|  10/10   |  0.010532  |  0.902915  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.010376  |  0.903667  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (2,100,tanh,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.009197  |  0.913167  |\n|  02/10   |  0.005645  |  0.945649  |\n|  03/10   |  0.004257  |  0.959966  |\n|  04/10   |  0.003598  |  0.965549  |\n|  05/10   |  0.003150  |  0.968316  |\n|  06/10   |  0.002896  |  0.971816  |\n|  07/10   |  0.002614  |  0.973683  |\n|  08/10   |  0.002442  |  0.975416  |\n|  09/10   |  0.002236  |  0.976750  |\n|  10/10   |  0.002168  |  0.977316  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001683  |  0.983133  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (2,100,sigmoid,SGD)    |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.070591  |  0.211900  |\n|  02/10   |  0.066180  |  0.527354  |\n|  03/10   |  0.055799  |  0.667242  |\n|  04/10   |  0.040097  |  0.751494  |\n|  05/10   |  0.028473  |  0.802396  |\n|  06/10   |  0.022356  |  0.830380  |\n|  07/10   |  0.019031  |  0.846281  |\n|  08/10   |  0.017012  |  0.858197  |\n|  09/10   |  0.015669  |  0.865348  |\n|  10/10   |  0.014709  |  0.871714  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.014302  |  0.874767  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (2,100,sigmoid,Adam)   |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.011247  |  0.893283  |\n|  02/10   |  0.007281  |  0.932498  |\n|  03/10   |  0.005011  |  0.952032  |\n|  04/10   |  0.003858  |  0.962833  |\n|  05/10   |  0.003213  |  0.968983  |\n|  06/10   |  0.002861  |  0.971816  |\n|  07/10   |  0.002579  |  0.974433  |\n|  08/10   |  0.002322  |  0.977650  |\n|  09/10   |  0.002130  |  0.978366  |\n|  10/10   |  0.002030  |  0.980016  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001776  |  0.982100  |\n+------------+------------+\n\n\n+------------------------------------+\n|     TRAINING: (2,100,relu,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.059881  |  0.533717  |\n|  02/10   |  0.025707  |  0.823326  |\n|  03/10   |  0.014989  |  0.870914  |\n|  04/10   |  0.012438  |  0.886115  |\n|  05/10   |  0.011267  |  0.894998  |\n|  06/10   |  0.010535  |  0.901098  |\n|  07/10   |  0.009985  |  0.906648  |\n|  08/10   |  0.009554  |  0.910115  |\n|  09/10   |  0.009180  |  0.913899  |\n|  10/10   |  0.008833  |  0.917665  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.008674  |  0.919967  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (2,100,relu,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.005362  |  0.948733  |\n|  02/10   |  0.002768  |  0.972932  |\n|  03/10   |  0.002275  |  0.977316  |\n|  04/10   |  0.001926  |  0.981466  |\n|  05/10   |  0.001701  |  0.982933  |\n|  06/10   |  0.001575  |  0.983650  |\n|  07/10   |  0.001434  |  0.985283  |\n|  08/10   |  0.001247  |  0.986766  |\n|  09/10   |  0.001200  |  0.987483  |\n|  10/10   |  0.001071  |  0.989133  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001064  |  0.988350  |\n+------------+------------+\n\n\n+------------------------------------+\n|     TRAINING: (2,150,tanh,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.051243  |  0.614450  |\n|  02/10   |  0.024171  |  0.822027  |\n|  03/10   |  0.016997  |  0.856714  |\n|  04/10   |  0.014373  |  0.873214  |\n|  05/10   |  0.013002  |  0.883548  |\n|  06/10   |  0.012141  |  0.889815  |\n|  07/10   |  0.011554  |  0.894415  |\n|  08/10   |  0.011112  |  0.898515  |\n|  09/10   |  0.010764  |  0.901032  |\n|  10/10   |  0.010486  |  0.903682  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.010330  |  0.904900  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (2,150,tanh,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.010040  |  0.907517  |\n|  02/10   |  0.006685  |  0.937832  |\n|  03/10   |  0.004659  |  0.955632  |\n|  04/10   |  0.003753  |  0.963383  |\n|  05/10   |  0.003261  |  0.967783  |\n|  06/10   |  0.002956  |  0.969849  |\n|  07/10   |  0.002700  |  0.972466  |\n|  08/10   |  0.002486  |  0.975283  |\n|  09/10   |  0.002332  |  0.976300  |\n|  10/10   |  0.002165  |  0.978133  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.002304  |  0.975750  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (2,150,sigmoid,SGD)    |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.070058  |  0.262400  |\n|  02/10   |  0.064953  |  0.519254  |\n|  03/10   |  0.053677  |  0.665125  |\n|  04/10   |  0.038751  |  0.754178  |\n|  05/10   |  0.028191  |  0.805913  |\n|  06/10   |  0.022431  |  0.830430  |\n|  07/10   |  0.019196  |  0.845547  |\n|  08/10   |  0.017199  |  0.855331  |\n|  09/10   |  0.015852  |  0.863231  |\n|  10/10   |  0.014893  |  0.869914  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.014479  |  0.873217  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (2,150,sigmoid,Adam)   |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.011428  |  0.891350  |\n|  02/10   |  0.006586  |  0.938915  |\n|  03/10   |  0.004551  |  0.956299  |\n|  04/10   |  0.003628  |  0.964549  |\n|  05/10   |  0.003109  |  0.969866  |\n|  06/10   |  0.002749  |  0.973033  |\n|  07/10   |  0.002554  |  0.975066  |\n|  08/10   |  0.002285  |  0.976733  |\n|  09/10   |  0.002166  |  0.977966  |\n|  10/10   |  0.001963  |  0.980083  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001427  |  0.986000  |\n+------------+------------+\n\n\n+------------------------------------+\n|     TRAINING: (2,150,relu,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.060776  |  0.528717  |\n|  02/10   |  0.025369  |  0.822559  |\n|  03/10   |  0.014646  |  0.871514  |\n|  04/10   |  0.012228  |  0.887498  |\n|  05/10   |  0.011132  |  0.895965  |\n|  06/10   |  0.010443  |  0.901698  |\n|  07/10   |  0.009935  |  0.906548  |\n|  08/10   |  0.009514  |  0.911115  |\n|  09/10   |  0.009134  |  0.914165  |\n|  10/10   |  0.008792  |  0.918265  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.008606  |  0.919450  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (2,150,relu,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.004991  |  0.952250  |\n|  02/10   |  0.002749  |  0.973916  |\n|  03/10   |  0.002267  |  0.977283  |\n|  04/10   |  0.001971  |  0.980683  |\n|  05/10   |  0.001765  |  0.982616  |\n|  06/10   |  0.001601  |  0.983450  |\n|  07/10   |  0.001480  |  0.984483  |\n|  08/10   |  0.001349  |  0.986116  |\n|  09/10   |  0.001258  |  0.986650  |\n|  10/10   |  0.001187  |  0.987783  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001160  |  0.987400  |\n+------------+------------+\n\n\n+------------------------------------+\n|     TRAINING: (3,100,tanh,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.062810  |  0.421400  |\n|  02/10   |  0.033909  |  0.739574  |\n|  03/10   |  0.019634  |  0.832312  |\n|  04/10   |  0.015395  |  0.862131  |\n|  05/10   |  0.013514  |  0.876898  |\n|  06/10   |  0.012439  |  0.886415  |\n|  07/10   |  0.011731  |  0.892381  |\n|  08/10   |  0.011221  |  0.896548  |\n|  09/10   |  0.010825  |  0.900065  |\n|  10/10   |  0.010499  |  0.902782  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.010324  |  0.904600  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (3,100,tanh,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.009998  |  0.905850  |\n|  02/10   |  0.006139  |  0.941632  |\n|  03/10   |  0.004378  |  0.957982  |\n|  04/10   |  0.003620  |  0.963933  |\n|  05/10   |  0.003109  |  0.969633  |\n|  06/10   |  0.002834  |  0.972499  |\n|  07/10   |  0.002554  |  0.974066  |\n|  08/10   |  0.002357  |  0.976466  |\n|  09/10   |  0.002145  |  0.977550  |\n|  10/10   |  0.002055  |  0.978916  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001907  |  0.981450  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (3,100,sigmoid,SGD)    |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.071512  |  0.134450  |\n|  02/10   |  0.069940  |  0.271986  |\n|  03/10   |  0.066187  |  0.431088  |\n|  04/10   |  0.056237  |  0.573807  |\n|  05/10   |  0.041236  |  0.711326  |\n|  06/10   |  0.028947  |  0.779845  |\n|  07/10   |  0.022318  |  0.810063  |\n|  08/10   |  0.018937  |  0.829780  |\n|  09/10   |  0.016988  |  0.843730  |\n|  10/10   |  0.015687  |  0.855747  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.015271  |  0.859217  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (3,100,sigmoid,Adam)   |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.011747  |  0.888967  |\n|  02/10   |  0.007287  |  0.932431  |\n|  03/10   |  0.005008  |  0.952182  |\n|  04/10   |  0.004109  |  0.959983  |\n|  05/10   |  0.003523  |  0.965316  |\n|  06/10   |  0.003104  |  0.969566  |\n|  07/10   |  0.002793  |  0.971916  |\n|  08/10   |  0.002610  |  0.974266  |\n|  09/10   |  0.002351  |  0.976483  |\n|  10/10   |  0.002265  |  0.977150  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.002152  |  0.977017  |\n+------------+------------+\n\n\n+------------------------------------+\n|     TRAINING: (3,100,relu,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.066579  |  0.346867  |\n|  02/10   |  0.034268  |  0.756156  |\n|  03/10   |  0.016701  |  0.849113  |\n|  04/10   |  0.013295  |  0.876264  |\n|  05/10   |  0.011916  |  0.888165  |\n|  06/10   |  0.011160  |  0.894065  |\n|  07/10   |  0.010655  |  0.898582  |\n|  08/10   |  0.010268  |  0.902815  |\n|  09/10   |  0.009929  |  0.906615  |\n|  10/10   |  0.009633  |  0.909598  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.009414  |  0.911600  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (3,100,relu,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.005735  |  0.945133  |\n|  02/10   |  0.003110  |  0.969516  |\n|  03/10   |  0.002602  |  0.974666  |\n|  04/10   |  0.002199  |  0.978183  |\n|  05/10   |  0.001966  |  0.980333  |\n|  06/10   |  0.001772  |  0.982650  |\n|  07/10   |  0.001679  |  0.983166  |\n|  08/10   |  0.001551  |  0.984283  |\n|  09/10   |  0.001423  |  0.985766  |\n|  10/10   |  0.001364  |  0.985733  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.000942  |  0.989917  |\n+------------+------------+\n\n\n+------------------------------------+\n|     TRAINING: (3,150,tanh,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.061582  |  0.495067  |\n|  02/10   |  0.030819  |  0.777675  |\n|  03/10   |  0.018188  |  0.849880  |\n|  04/10   |  0.014637  |  0.871564  |\n|  05/10   |  0.013062  |  0.882448  |\n|  06/10   |  0.012150  |  0.888231  |\n|  07/10   |  0.011525  |  0.893665  |\n|  08/10   |  0.011060  |  0.898198  |\n|  09/10   |  0.010693  |  0.901315  |\n|  10/10   |  0.010385  |  0.903998  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.010221  |  0.905483  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (3,150,tanh,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.010190  |  0.905267  |\n|  02/10   |  0.006159  |  0.941048  |\n|  03/10   |  0.004561  |  0.955332  |\n|  04/10   |  0.003812  |  0.962683  |\n|  05/10   |  0.003392  |  0.966583  |\n|  06/10   |  0.003026  |  0.970199  |\n|  07/10   |  0.002750  |  0.972350  |\n|  08/10   |  0.002551  |  0.973983  |\n|  09/10   |  0.002330  |  0.976516  |\n|  10/10   |  0.002318  |  0.975766  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001803  |  0.981100  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (3,150,sigmoid,SGD)    |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.071322  |  0.132833  |\n|  02/10   |  0.069418  |  0.310252  |\n|  03/10   |  0.064532  |  0.521972  |\n|  04/10   |  0.051747  |  0.643975  |\n|  05/10   |  0.035716  |  0.740927  |\n|  06/10   |  0.025385  |  0.794946  |\n|  07/10   |  0.020292  |  0.821697  |\n|  08/10   |  0.017680  |  0.838214  |\n|  09/10   |  0.016116  |  0.850947  |\n|  10/10   |  0.015046  |  0.860814  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.014570  |  0.865817  |\n+------------+------------+\n\n\n+------------------------------------+\n|   TRAINING: (3,150,sigmoid,Adam)   |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.012345  |  0.883783  |\n|  02/10   |  0.007806  |  0.926698  |\n|  03/10   |  0.005364  |  0.949199  |\n|  04/10   |  0.004136  |  0.961099  |\n|  05/10   |  0.003663  |  0.964483  |\n|  06/10   |  0.003147  |  0.969549  |\n|  07/10   |  0.002898  |  0.971683  |\n|  08/10   |  0.002605  |  0.974750  |\n|  09/10   |  0.002455  |  0.975933  |\n|  10/10   |  0.002192  |  0.977716  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.001820  |  0.981900  |\n+------------+------------+\n\n\n+------------------------------------+\n|     TRAINING: (3,150,relu,SGD)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.067894  |  0.364350  |\n|  02/10   |  0.037720  |  0.756189  |\n|  03/10   |  0.016626  |  0.857146  |\n|  04/10   |  0.012854  |  0.882448  |\n|  05/10   |  0.011453  |  0.893531  |\n|  06/10   |  0.010650  |  0.900432  |\n|  07/10   |  0.010087  |  0.905315  |\n|  08/10   |  0.009633  |  0.909215  |\n|  09/10   |  0.009229  |  0.913198  |\n|  10/10   |  0.008852  |  0.917599  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.008601  |  0.921000  |\n+------------+------------+\n\n\n+------------------------------------+\n|    TRAINING: (3,150,relu,Adam)     |\n+----------+------------+------------+\n|  Epoch   |    Loss    |  Accuracy  |\n+----------+------------+------------+\n|  01/10   |  0.005707  |  0.944383  |\n|  02/10   |  0.003217  |  0.968449  |\n|  03/10   |  0.002656  |  0.973266  |\n|  04/10   |  0.002313  |  0.977500  |\n|  05/10   |  0.002077  |  0.979266  |\n|  06/10   |  0.001870  |  0.981383  |\n|  07/10   |  0.001743  |  0.982316  |\n|  08/10   |  0.001576  |  0.983933  |\n|  09/10   |  0.001536  |  0.984266  |\n|  10/10   |  0.001411  |  0.985383  |\n+----------+------------+------------+\n\n+-------------------------+\n|         TESTING         |\n+------------+------------+\n|    Loss    |  Accuracy  |\n+------------+------------+\n|  0.000886  |  0.990617  |\n+------------+------------+\n\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
